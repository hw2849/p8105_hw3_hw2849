---
title: "p8105_hw3_hw2849"
author: hw2849
date: 2021.10.13
output: github_document
---

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(timeDate)
library(p8105.datasets)
data("instacart")
data("brfss_smart2010")

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Problem 1
```{r}
instacart %>% 
  group_by(aisle_id) %>% 
  
  ## count for how many aisles and orders in each aisle
  summarize(order_counts = n()) %>% 
  
  ## ranking aisles by the number of orders
  mutate(
    ranking = min_rank(desc(order_counts))
    ) %>% 
  
  ## find the aisle of the most items ordered from
  filter(min_rank(ranking) < 2)


## Sort aisles with 10000 more orders
plot_df = instacart %>% 
  count(aisle, name = "orders_over_10000") %>% ## count for variable
  filter(orders_over_10000 > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, orders_over_10000)
  ) %>% 
  ggplot(aes(x = aisle, y = orders_over_10000)) + 
  geom_point() + 
  labs(
    title = "Figure 1: Aisles of over 10000 items ordered",
    x = "Aisle",
    y = "Number of items ordered"
  ) + 
  theme(axis.text.x = element_text(angle = 90))

plot_df

## A table for three most popular items in “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
table = instacart %>% 
  group_by(aisle) %>% 
  filter(
    aisle %in% c("baking ingredients", 
                 "dog food care", 
                 "packaged vegetables fruits")
    ) %>% 
  count(product_name, name = "counts") %>% 
  mutate(
    ranking = min_rank(desc(counts))
  ) %>% 
  filter(ranking < 4)

table

## A table for mean hour of the day Pink Lady Apples and Coffee Ice Cream
table_mean_hour = instacart %>% 
  group_by(product_name, order_dow) %>% 
  filter(
    product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")
      ) %>% 
  summarize(
    mean_hour = mean(order_hour_of_day)
    ) %>% 
 mutate(
   order_dow = lubridate::wday(order_dow + 1, label = TRUE)
   ) %>% 
  pivot_wider(
  names_from = order_dow,
  values_from = mean_hour
  )

table_mean_hour

```

First, we have 1384617 observations with 15 variables in `instacart` dataset, which  including user ids, product names, product aisles, aisles numbers, the day of the week and the hour of the day on which the order was placed. 

There are 134 aisles, and the most items ordered from aisle 83, which is fresh vegetables. The plot shows the number of items ordered in aisles that are more than 10000 items ordered. 

The table shows three most popular items in aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. For example,top three popular items of "baking ingredients" are light brown sugar, pure baking soda, and cane sugar. 

The last table summarizes the mean hour of the day that "Pink Lady Apples" and "Coffee Ice Cream" are ordered on each day of the week. For instance, the mean hour at "Coffee Ice Cream" are ordered on day "3" is 15.31818. 

## Problem 2
```{r}
## Data cleaning on `Overall Health` topic
health_data_df = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, county = locationdesc) %>% 
  group_by(topic) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(
    response = factor(response, 
                      levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))
    ) %>% 
  arrange(response, .by_group = TRUE) 

```

* In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
## 7 or more locations of a state observed in 2002
observation_2002 = health_data_df %>% 
  filter(year == "2002") %>% 
  count(state, name = "state_counts") %>% 
  filter(state_counts >= 7)

## 7 or more locations of a state observed in 2010
observation_2010 = health_data_df %>% 
  filter(year == "2010") %>% 
  count(state, name = "state_counts") %>% 
  filter(state_counts >= 7)

```

* A data set limits to `Excellent` responses, and contains year, state, and mean of `data_value` across locations within a state. A lined-plot of the mean value over time within a state.

```{r}
mean_data_value = health_data_df %>% 
  group_by(year, state) %>% 
  filter(response == "Excellent") %>% 
  summarize(mean_value = mean(data_value)) ##  pivot_wider(names_from = year, values_from = mean_value) 
  
## A spaghetti plot that shows lines for the mean value of each state across years
spaghetti_plot = ggplot(mean_data_value) + 
  geom_line(aes(x = year, y = mean_value, color = state)) + 
  labs(
    title = "Figure 2: Average value across locations within each state over years",
    x = "State",
    y = "Average value"
  )

spaghetti_plot
```

* Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.
```{r}
NY_response_df = health_data_df %>%
  filter(
    state == "NY", 
    year %in% c(2006,2010)
    ) %>% 
  ggplot(aes(x = response, y = data_value)) + 
  geom_boxplot() + 
  facet_grid(.~year) + 
  labs(
    title = "Figure 3: Data value across responses in NY State", 
    x = "Response levels",
    y = "Data Value"
  ) 

NY_response_df
```

In this problem, we focused on the topic of 'Overall Health'. Firstly, changed the variable names, for example, 'locationabbr' was changed into 'state. Then, values of variable 'response' was reordered in levels of {Poor, Fair, Good, Very good, Excellent}. 

Looking at the data frame, in 2002, there were 36 states were observed at 7 or more locations. However, in 2010, there were 45 states were observed. 

Next, we constructed a dataset that limits to 'Excellent' responses, and grouped by 'year', 'state', and created a variable 'mean_value' that indicates the average values of 'data_value' across locations within a state. Graphed a spaghetti plot that shows lines for the mean value of each state across years. 

Lastly, generated a two-panel plot showing the distribution of 'data_value' for response in 2006 and 2010 for NY state. 

## Problem 3

*Load and tidy the data, which includes all originallt variables and values, and a new variable `weekday vs weekend`.
```{r}
accelerometer_data = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% ## clean variable names
  pivot_longer( ## make the table readable
    activity_1:activity_1440,
    names_to = "activity_time",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate( ## refactor the variables
    day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")), 
    week = factor(week), 
    weekday_or_weekend = ifelse(day == "Sunday" | day == "Saturday", print("Weekend"), print("Weekday")), ## a new variable whether weekend/weekday
    activity_time = as.numeric(activity_time)
    ) %>% 
  arrange(week, day) ## reorder the variables by `day`
```
Cleaned the variable names and used `pivot_longer` to make the table more readable. Created a weekday vs weekend variable. As a result, we have a table with 6 variables and 50400 observations. For example, we have variables tell that weeks, days of a week, and `activity_count` shows how long of the activity lasts for each day. 

* Find total activity and create a table showing these totals. Any trends..?
```{r}
total_activity = accelerometer_data %>% 
  group_by(day_id) %>% 
  summarize(total_time = sum(activity_count))

total_activity %>% 
  ggplot(aes(x = day_id, y = total_time)) + 
  geom_point() + 
  geom_line() + 
  labs(
    title = "Figure 4: Total acticity for each day"
  ) 
```
We grouped by `day_id` and look for the total activity for each day by `summarize`, and plot these data into a line plot but there was no trend observed. 

* Now we want to make a plot that shows the 24-hour activity time. 
```{r}
accelerometer_data %>% 
  ggplot(aes(x = activity_time, 
             y = activity_count, 
             group = day_id, 
             color = day)
         ) +  
  geom_smooth(aes(group = day)) + 
  scale_x_continuous(
    breaks = (0:23)*60 + 1,
    labels = c(0:23),
    name = "hour in a day"
  ) +  
  labs(
    title = "Figure 5: 24-hour activity overview for each day"
  ) 
```
This part we made a plot to show a 24-hour activity time for each day. The plot tells that Friday has a peak time of activity and Sunday follows. Generally between 0am to 5am there is less activity, and it starts increasing at 5am. And it has an obvious declining after 9pm-10pm. 
